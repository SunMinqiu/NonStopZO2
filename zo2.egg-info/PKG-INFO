Metadata-Version: 2.1
Name: zo2
Version: 0.1.1
Summary: ZO2 (Zeroth-Order Offloading), a framework for full parameter fine-tuning 175B LLMs with 18GB GPU memory
Author: liangyuwang
Author-email: liangyu.wang@kaust.edu.sa
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.11
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: brotli==1.0.9
Requires-Dist: certifi==2024.7.4
Requires-Dist: charset-normalizer==3.3.2
Requires-Dist: filelock==3.13.1
Requires-Dist: idna==3.7
Requires-Dist: Jinja2==3.1.4
Requires-Dist: MarkupSafe==2.1.3
Requires-Dist: numpy==1.26.4
Requires-Dist: Pillow==10.4.0
Requires-Dist: PySocks==1.7.1
Requires-Dist: PyYAML==6.0.1
Requires-Dist: requests==2.32.3
Requires-Dist: rich==14.0.0
Requires-Dist: setuptools==72.1.0
Requires-Dist: urllib3==2.2.2
Requires-Dist: wheel==0.43.0
Requires-Dist: accelerate==1.6.0
Requires-Dist: datasets==3.5.1
Requires-Dist: aiohttp==3.10.3
Requires-Dist: aiosignal==1.3.1
Requires-Dist: attrs==24.2.0
Requires-Dist: dill==0.3.8
Requires-Dist: frozenlist==1.4.1
Requires-Dist: fsspec==2024.5.0
Requires-Dist: joblib==1.4.2
Requires-Dist: multidict==6.0.5
Requires-Dist: multiprocess==0.70.16
Requires-Dist: opt-einsum==3.3.0
Requires-Dist: packaging==24.1
Requires-Dist: pandas==2.2.2
Requires-Dist: psutil==6.0.0
Requires-Dist: pyarrow==17.0.0
Requires-Dist: pyarrow-hotfix==0.6
Requires-Dist: python-dateutil==2.9.0.post0
Requires-Dist: pytz==2024.1
Requires-Dist: regex==2024.7.24
Requires-Dist: scikit-learn==1.5.1
Requires-Dist: scipy==1.14.0
Requires-Dist: six==1.16.0
Requires-Dist: threadpoolctl==3.5.0
Requires-Dist: tokenizers==0.21.1
Requires-Dist: tqdm==4.66.5
Requires-Dist: transformers==4.51.3
Requires-Dist: tzdata==2024.1
Requires-Dist: xxhash==3.4.1
Requires-Dist: yarl==1.9.4
Requires-Dist: nvidia-ml-py==12.570.86
Requires-Dist: trl==0.17.0
Requires-Dist: safetensors==0.5.2

# ZO2 (Zeroth-Order Offloading): Full Parameter Fine-Tuning 175B LLMs with 18GB GPU Memory

[![arXiv](https://img.shields.io/badge/Arxiv-2503.12668-b31b1b.svg?logo=arXiv)](https://arxiv.org/abs/2503.12668)
[![License](https://img.shields.io/badge/License-Apache%202.0-yellow)](https://github.com/liangyuwang/zo2/blob/main/LICENSE)
[![GitDiagran](https://img.shields.io/badge/Git-Diagram%20-blue)](https://gitdiagram.com/liangyuwang/zo2)
[![DeepWiki](https://img.shields.io/badge/Devin-DeepWiki%20-green)](https://deepwiki.com/liangyuwang/zo2)
<!-- <a target="_blank" href="https://colab.research.google.com/github/liangyuwang/zo2/blob/main/tutorial/colab.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a> -->

ğŸ‘‹ Welcome! **ZO2** is an innovative framework specifically designed to enhance the fine-tuning of large language models (LLMs) using **zeroth-order (ZO)** optimization techniques and advanced **offloading** technologies. This framework is particularly tailored for setups with limited GPU memory (e.g. fine-tune **[OPT-175B](https://arxiv.org/abs/2205.01068)** with just **18GB GPU memory**), enabling the fine-tuning of models that were previously unmanageable due to hardware constraints.

- The table below displays the GPU memory usage for various OPT model sizes when fine-tuned using the ZO2 framework:

|        OPT Models        |   1.3B   |   2.7B   |   6.7B   |   13B   |   30B   |    66B    |        175B        |
| :-----------------------: | :------: | :------: | :------: | :------: | :------: | :-------: | :-----------------: |
| **GPU memory (GB)** | `3.75` | `4.14` | `4.99` | `6.18` | `8.86` | `12.07` | **`18.04`** |

- [Install](#ï¸installation) the package and execute the following test to see the memory usage:

```shell
  bash test/mezo_sgd/hf_opt/record_zo2_memory.sh
```

## ğŸ“° News

- 16/07/2025: ZO2 is accepted by [COLM](https://colmweb.org/index.html).
- 02/05/2025: Added support for [Qwen3](https://qwenlm.github.io/blog/qwen3/). You can now fully fine-tune the [32B version](https://huggingface.co/Qwen/Qwen3-32B-FP8) with just 6GB GPU memory using ZO2. Please refer to our [example](example/mezo_runner/).
- 01/05/2025: We upgraded the environment and dependencies to align with the latest `transformers==4.51.3`. 
- 06/03/2025: We have open-sourced ZO2!

## ğŸ’¡ Key Features

- **Optimized ZO CPU Offloading**: ZO2 leverages `zeroth-order (ZO)` methods to efficiently use `CPU offloading`, avoiding redundant data transfers and significantly reducing GPU memory demands. This allows for handling large-scale models on hardware with limited GPU resources.
- **Dynamic Scheduling**: Incorporates a high-performance scheduler to optimize the `computation-communication overlap`, enhancing GPU utilization and preventing training delays.
- **Capability for Very Large Models**: Enables the fine-tuning of extraordinarily large models, such as those with over `175 billion parameters`, on single GPUs with as little as `18GB` of memory, previously impossible with traditional methods.
- **Empirical Validation**: ZO2 has demonstrated through rigorous testing that it can efficiently fine-tune massive models `without extra time costs or accuracy losses`, confirming its effectiveness for large-scale model training.

## âš™ï¸ Installation

We offer two installation options, and you only need to use one of them to install ZO2:

1. To experiment with our examples, tutorials, or tests, follow these steps to set up the ZO2 environment:

```shell
  git clone https://github.com/liangyuwang/zo2.git
  cd zo2/
  conda env create -f env.yml
  conda activate zo2
```

2. If you want to use ZO2 as a package in your own code, you can install it directly in your Python environment.

    Before installing the ZO2 package, ensure you have the required dependencies:

    - [PyTorch](https://pytorch.org/get-started/locally/) >= 2.4.0, CUDA >= 12.1

    Once the dependencies are installed, you can install the ZO2 package using pip:

```shell
  pip install git+https://github.com/liangyuwang/zo2.git
```

## ğŸ› ï¸ Usage

We utilize the [OPT](https://arxiv.org/abs/2205.01068) models and [MeZO-SGD](https://arxiv.org/abs/2305.17333) as examples. For additional information, please refer to the section on [Supported Models and ZO methods](#-supported-models-zo-methods-and-tasks-support).

### 1. Using [MeZO-Runner](example/mezo_runner/) to Evaluate Fine-tuning Tasks

Before running the following commands, please ensure that you have cloned the entire project. If you [installed](#ï¸installation) ZO2 using option 2, you will need to run "git clone https://github.com/liangyuwang/zo2.git" to obtain the complete project, then navigate to the zo2 folder by "cd zo2".

```shell
cd example/mezo_runner/
export CUDA_VISIBLE_DEVICES=0
MODEL=facebook/opt-2.7b TASK=SST2 MODE=ft LR=1e-7 EPS=1e-3 STEPS=20000 EVAL_STEPS=4000 bash mezo.sh
```

### 2. Fine-Tuning HF Models with ZOTrainer / ZOSFTTrainer [[Trainer](./tutorial/huggingface.ipynb)]

```python
from zo2 import ZOConfig, zo_hf_init
from zo2.trainer.hf_transformers import ZOTrainer
from transformers import TrainingArguments

# Model and optimizer init
zo_config = ZOConfig(method="mezo-sgd", zo2=True, offloading_device='cpu', working_device='cuda', lr=1e-5)
with zo_hf_init(zo_config):
Â  Â  from transformers import OPTForCausalLM
Â  Â  model = OPTForCausalLM.from_pretrained("facebook/opt-125m")
Â  Â  model.zo_init(zo_config)

training_args = TrainingArguments("test-trainer")

trainer = ZOTrainer(
    model,
    args = training_args,
    train_dataset=...,   # get training dataset
    eval_dataset=...,    # get eval dataset
    data_collator=...,   # get data_collator
    tokenizer=...,       # use suitable tokenizer
    ...
)

trainer.train()
```

### 3. Train HF Models with Custom Training Loop [[demo](./tutorial/demo.ipynb)]

```python
from zo2 import ZOConfig, zo_hf_init

# Model and optimizer init
zo_config = ZOConfig(method="mezo-sgd", zo2=True, offloading_device='cpu', working_device='cuda', lr=1e-5)
with zo_hf_init(zo_config):
Â  Â  from transformers import OPTForCausalLM
Â  Â  model = OPTForCausalLM.from_pretrained("facebook/opt-125m")
Â  Â  model.zo_init(zo_config)

# Training loop
for i in range(max_training_step):
Â  Â  # Train
Â  Â  training_input_ids, training_labels = ... Â  # get training data batch
Â  Â  model.zo_train()
Â  Â  loss = model(input_ids=training_input_ids, labels=training_labels)
Â  Â  # Evaluate
Â  Â  eval_input_ids, eval_labels = ... Â  # get eval data batch
Â  Â  model.zo_eval() Â  Â  
Â  Â  output = model(input_ids=eval_input_ids, labels=eval_labels)
```

## âœ¨ Tutorial

Please refer to [tutorial](./tutorial/).

## ğŸ¤– Supported Models, ZO methods, and Tasks

- **Models**:

  * [NanoGPT](https://github.com/karpathy/build-nanogpt/blob/master/train_gpt2.py)   (mainly for idea evaluation)
  * [Transformers](https://github.com/huggingface/transformers): [OPT](https://arxiv.org/abs/2205.01068)
- **ZO methods**:

  * [MeZO-SGD](https://arxiv.org/abs/2305.17333)
- **Tasks**: Please refer to [MeZO-Runner](example/mezo_runner/)

## ğŸ§ª Test

Please refer to [test](./test/).

## ğŸ§­ Roadmap

- [ ] Support more models like LLaMA and DeepSeek
- [ ] Support more ZO methods
- [ ] Support more offloading strategies (Disk offloading)

## ğŸš¶ Contributing

Feel free to submit issues and pull requests to improve the project!

## ğŸ“² Contact

* Liangyu Wang: liangyu.wang@kaust.edu.sa

## ğŸ“– BibTeX

```
@article{wang2025zo2,
  title={ZO2: Scalable Zeroth-Order Fine-Tuning for Extremely Large Language Models with Limited GPU Memory},
  author={Wang, Liangyu and Ren, Jie and Xu, Hang and Wang, Junxiao and Xie, Huanyi and Keyes, David E and Wang, Di},
  journal={arXiv preprint arXiv:2503.12668},
  year={2025}
}
```
